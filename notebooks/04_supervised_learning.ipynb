{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Supervised Learning - Classification Models\n",
        "\n",
        "This notebook covers:\n",
        "- Training multiple classification models\n",
        "- Model evaluation and comparison\n",
        "- Performance metrics analysis\n",
        "- ROC curves and AUC scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
        "                           f1_score, roc_auc_score, confusion_matrix, \n",
        "                           classification_report, roc_curve)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the selected features and target\n",
        "X_selected = pd.read_csv('data/X_selected.csv')\n",
        "y = pd.read_csv('data/y_target.csv').values.ravel()\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Features shape: {X_selected.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "print(f\"Selected features: {list(X_selected.columns)}\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n",
        "print(f\"Train target distribution: {np.bincount(y_train)}\")\n",
        "print(f\"Test target distribution: {np.bincount(y_test)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "trained_models = {}\n",
        "\n",
        "print(\"Training and Evaluating Models:\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name}...\")\n",
        "    \n",
        "    # Train the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    precision = precision_score(y_test, y_pred)\n",
        "    recall = recall_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    auc = roc_auc_score(y_test, y_pred_proba) if y_pred_proba is not None else None\n",
        "    \n",
        "    # Store results\n",
        "    results[name] = {\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'auc': auc,\n",
        "        'predictions': y_pred,\n",
        "        'probabilities': y_pred_proba\n",
        "    }\n",
        "    trained_models[name] = model\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-Score:  {f1:.4f}\")\n",
        "    if auc is not None:\n",
        "        print(f\"AUC:       {auc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create performance comparison table\n",
        "print(\"\\nModel Performance Comparison:\")\n",
        "print(\"=\" * 35)\n",
        "\n",
        "# Create results DataFrame\n",
        "results_df = pd.DataFrame({\n",
        "    name: {\n",
        "        'Accuracy': results[name]['accuracy'],\n",
        "        'Precision': results[name]['precision'],\n",
        "        'Recall': results[name]['recall'],\n",
        "        'F1-Score': results[name]['f1'],\n",
        "        'AUC': results[name]['auc'] if results[name]['auc'] is not None else 'N/A'\n",
        "    } for name in results.keys()\n",
        "}).T\n",
        "\n",
        "print(results_df.round(4))\n",
        "\n",
        "# Find best model by F1-score\n",
        "best_model_name = results_df['F1-Score'].idxmax()\n",
        "print(f\"\\nBest model by F1-Score: {best_model_name}\")\n",
        "print(f\"Best F1-Score: {results_df.loc[best_model_name, 'F1-Score']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model performance\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# 1. Performance metrics comparison\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "x = np.arange(len(metrics))\n",
        "width = 0.2\n",
        "\n",
        "for i, (model_name, _) in enumerate(models.items()):\n",
        "    values = [results_df.loc[model_name, metric] for metric in metrics]\n",
        "    axes[0, 0].bar(x + i*width, values, width, label=model_name)\n",
        "\n",
        "axes[0, 0].set_xlabel('Metrics')\n",
        "axes[0, 0].set_ylabel('Score')\n",
        "axes[0, 0].set_title('Model Performance Comparison')\n",
        "axes[0, 0].set_xticks(x + width * 1.5)\n",
        "axes[0, 0].set_xticklabels(metrics)\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Confusion matrices\n",
        "for i, (name, _) in enumerate(models.items()):\n",
        "    cm = confusion_matrix(y_test, results[name]['predictions'])\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 1] if i == 0 else None)\n",
        "    if i == 0:\n",
        "        axes[0, 1].set_title(f'Confusion Matrix - {name}')\n",
        "        axes[0, 1].set_xlabel('Predicted')\n",
        "        axes[0, 1].set_ylabel('Actual')\n",
        "\n",
        "# 3. ROC Curves\n",
        "for name, result in results.items():\n",
        "    if result['probabilities'] is not None:\n",
        "        fpr, tpr, _ = roc_curve(y_test, result['probabilities'])\n",
        "        auc = result['auc']\n",
        "        axes[1, 0].plot(fpr, tpr, label=f'{name} (AUC = {auc:.3f})')\n",
        "\n",
        "axes[1, 0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "axes[1, 0].set_xlabel('False Positive Rate')\n",
        "axes[1, 0].set_ylabel('True Positive Rate')\n",
        "axes[1, 0].set_title('ROC Curves')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Feature importance (for tree-based models)\n",
        "if 'Random Forest' in trained_models:\n",
        "    rf_model = trained_models['Random Forest']\n",
        "    feature_importance = rf_model.feature_importances_\n",
        "    feature_names = X_selected.columns\n",
        "    \n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=True)\n",
        "    \n",
        "    axes[1, 1].barh(importance_df['feature'], importance_df['importance'])\n",
        "    axes[1, 1].set_xlabel('Importance')\n",
        "    axes[1, 1].set_title('Random Forest Feature Importance')\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-validation evaluation\n",
        "print(\"Cross-Validation Results:\")\n",
        "print(\"=\" * 25)\n",
        "\n",
        "cv_results = {}\n",
        "for name, model in models.items():\n",
        "    # Perform 5-fold cross-validation\n",
        "    cv_scores = cross_val_score(model, X_selected, y, cv=5, scoring='f1')\n",
        "    cv_results[name] = cv_scores\n",
        "    \n",
        "    print(f\"{name:20}: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
        "\n",
        "# Visualize cross-validation results\n",
        "plt.figure(figsize=(10, 6))\n",
        "cv_df = pd.DataFrame(cv_results)\n",
        "cv_df.boxplot()\n",
        "plt.title('Cross-Validation F1-Scores')\n",
        "plt.ylabel('F1-Score')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the best model\n",
        "best_model = trained_models[best_model_name]\n",
        "print(f\"\\nBest model ({best_model_name}) saved for further use.\")\n",
        "print(f\"Final performance on test set:\")\n",
        "print(f\"Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
        "print(f\"F1-Score: {results[best_model_name]['f1']:.4f}\")\n",
        "print(f\"AUC: {results[best_model_name]['auc']:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
