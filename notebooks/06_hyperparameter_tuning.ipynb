{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hyperparameter Tuning\n",
        "\n",
        "This notebook covers:\n",
        "- GridSearchCV for systematic parameter search\n",
        "- RandomizedSearchCV for efficient parameter exploration\n",
        "- Model optimization and comparison\n",
        "- Best parameter selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set style for better plots\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the data\n",
        "X_selected = pd.read_csv('data/X_selected.csv')\n",
        "y = pd.read_csv('data/y_target.csv').values.ravel()\n",
        "\n",
        "print(\"Data loaded successfully!\")\n",
        "print(f\"Features shape: {X_selected.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_selected, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTrain set: {X_train.shape[0]} samples\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter grids for each model\n",
        "param_grids = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga'],\n",
        "        'max_iter': [1000, 2000]\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': [3, 5, 7, 10, None],\n",
        "        'min_samples_split': [2, 5, 10, 20],\n",
        "        'min_samples_leaf': [1, 2, 4, 8],\n",
        "        'criterion': ['gini', 'entropy']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 200, 300],\n",
        "        'max_depth': [3, 5, 7, 10, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4],\n",
        "        'bootstrap': [True, False]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "        'kernel': ['rbf', 'linear', 'poly'],\n",
        "        'degree': [2, 3, 4]\n",
        "    }\n",
        "}\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'SVM': SVC(random_state=42, probability=True)\n",
        "}\n",
        "\n",
        "print(\"Parameter grids defined for hyperparameter tuning\")\n",
        "print(\"=\" * 50)\n",
        "for model_name, params in param_grids.items():\n",
        "    print(f\"{model_name}: {len(params)} parameter categories\")\n",
        "    for param, values in params.items():\n",
        "        print(f\"  {param}: {values}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. GridSearchCV for systematic parameter search\n",
        "print(\"1. GridSearchCV Hyperparameter Tuning:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "grid_results = {}\n",
        "grid_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nTuning {model_name}...\")\n",
        "    \n",
        "    # Create GridSearchCV\n",
        "    grid_search = GridSearchCV(\n",
        "        estimator=model,\n",
        "        param_grid=param_grids[model_name],\n",
        "        cv=5,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Fit the grid search\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Store results\n",
        "    grid_results[model_name] = {\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'best_score': grid_search.best_score_,\n",
        "        'best_estimator': grid_search.best_estimator_\n",
        "    }\n",
        "    grid_models[model_name] = grid_search.best_estimator_\n",
        "    \n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best CV score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\nGridSearchCV completed for all models!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. RandomizedSearchCV for efficient parameter exploration\n",
        "print(\"\\n2. RandomizedSearchCV Hyperparameter Tuning:\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "# Create smaller parameter distributions for RandomizedSearchCV\n",
        "random_param_distributions = {\n",
        "    'Logistic Regression': {\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'penalty': ['l1', 'l2'],\n",
        "        'solver': ['liblinear', 'saga']\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'max_depth': [3, 5, 7, 10, None],\n",
        "        'min_samples_split': [2, 5, 10, 20],\n",
        "        'min_samples_leaf': [1, 2, 4, 8],\n",
        "        'criterion': ['gini', 'entropy']\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'n_estimators': [50, 100, 200, 300],\n",
        "        'max_depth': [3, 5, 7, 10, None],\n",
        "        'min_samples_split': [2, 5, 10],\n",
        "        'min_samples_leaf': [1, 2, 4]\n",
        "    },\n",
        "    'SVM': {\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1],\n",
        "        'kernel': ['rbf', 'linear', 'poly']\n",
        "    }\n",
        "}\n",
        "\n",
        "random_results = {}\n",
        "random_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nRandom tuning {model_name}...\")\n",
        "    \n",
        "    # Create RandomizedSearchCV\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=model,\n",
        "        param_distributions=random_param_distributions[model_name],\n",
        "        n_iter=20,  # Number of parameter settings sampled\n",
        "        cv=5,\n",
        "        scoring='f1',\n",
        "        n_jobs=-1,\n",
        "        random_state=42,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    # Fit the random search\n",
        "    random_search.fit(X_train, y_train)\n",
        "    \n",
        "    # Store results\n",
        "    random_results[model_name] = {\n",
        "        'best_params': random_search.best_params_,\n",
        "        'best_score': random_search.best_score_,\n",
        "        'best_estimator': random_search.best_estimator_\n",
        "    }\n",
        "    random_models[model_name] = random_search.best_estimator_\n",
        "    \n",
        "    print(f\"Best parameters: {random_search.best_params_}\")\n",
        "    print(f\"Best CV score: {random_search.best_score_:.4f}\")\n",
        "\n",
        "print(\"\\nRandomizedSearchCV completed for all models!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Compare GridSearchCV vs RandomizedSearchCV results\n",
        "print(\"\\n3. Hyperparameter Tuning Comparison:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_data = []\n",
        "for model_name in models.keys():\n",
        "    grid_score = grid_results[model_name]['best_score']\n",
        "    random_score = random_results[model_name]['best_score']\n",
        "    \n",
        "    comparison_data.append({\n",
        "        'Model': model_name,\n",
        "        'GridSearchCV Score': grid_score,\n",
        "        'RandomizedSearchCV Score': random_score,\n",
        "        'Difference': grid_score - random_score,\n",
        "        'Best Method': 'GridSearchCV' if grid_score > random_score else 'RandomizedSearchCV'\n",
        "    })\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "print(comparison_df.round(4))\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Scores comparison\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "grid_scores = [grid_results[name]['best_score'] for name in models.keys()]\n",
        "random_scores = [random_results[name]['best_score'] for name in models.keys()]\n",
        "\n",
        "axes[0].bar(x - width/2, grid_scores, width, label='GridSearchCV', alpha=0.8)\n",
        "axes[0].bar(x + width/2, random_scores, width, label='RandomizedSearchCV', alpha=0.8)\n",
        "axes[0].set_xlabel('Models')\n",
        "axes[0].set_ylabel('F1 Score')\n",
        "axes[0].set_title('GridSearchCV vs RandomizedSearchCV')\n",
        "axes[0].set_xticks(x)\n",
        "axes[0].set_xticklabels(models.keys(), rotation=45)\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Difference plot\n",
        "differences = [grid_scores[i] - random_scores[i] for i in range(len(models))]\n",
        "colors = ['green' if diff > 0 else 'red' for diff in differences]\n",
        "axes[1].bar(models.keys(), differences, color=colors, alpha=0.7)\n",
        "axes[1].set_xlabel('Models')\n",
        "axes[1].set_ylabel('Score Difference (Grid - Random)')\n",
        "axes[1].set_title('GridSearchCV Advantage')\n",
        "axes[1].tick_params(axis='x', rotation=45)\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "axes[1].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4. Final Model Evaluation\n",
        "print(\"\\n4. Final Model Evaluation on Test Set:\")\n",
        "print(\"=\" * 45)\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Evaluate both GridSearchCV and RandomizedSearchCV models\n",
        "final_results = {}\n",
        "\n",
        "for model_name in models.keys():\n",
        "    print(f\"\\n{model_name}:\")\n",
        "    print(\"-\" * 20)\n",
        "    \n",
        "    # GridSearchCV model\n",
        "    grid_model = grid_models[model_name]\n",
        "    grid_pred = grid_model.predict(X_test)\n",
        "    grid_pred_proba = grid_model.predict_proba(X_test)[:, 1] if hasattr(grid_model, 'predict_proba') else None\n",
        "    \n",
        "    grid_accuracy = accuracy_score(y_test, grid_pred)\n",
        "    grid_precision = precision_score(y_test, grid_pred)\n",
        "    grid_recall = recall_score(y_test, grid_pred)\n",
        "    grid_f1 = f1_score(y_test, grid_pred)\n",
        "    grid_auc = roc_auc_score(y_test, grid_pred_proba) if grid_pred_proba is not None else None\n",
        "    \n",
        "    print(f\"GridSearchCV - Accuracy: {grid_accuracy:.4f}, F1: {grid_f1:.4f}, AUC: {grid_auc:.4f if grid_auc else 'N/A'}\")\n",
        "    \n",
        "    # RandomizedSearchCV model\n",
        "    random_model = random_models[model_name]\n",
        "    random_pred = random_model.predict(X_test)\n",
        "    random_pred_proba = random_model.predict_proba(X_test)[:, 1] if hasattr(random_model, 'predict_proba') else None\n",
        "    \n",
        "    random_accuracy = accuracy_score(y_test, random_pred)\n",
        "    random_precision = precision_score(y_test, random_pred)\n",
        "    random_recall = recall_score(y_test, random_pred)\n",
        "    random_f1 = f1_score(y_test, random_pred)\n",
        "    random_auc = roc_auc_score(y_test, random_pred_proba) if random_pred_proba is not None else None\n",
        "    \n",
        "    print(f\"RandomizedSearchCV - Accuracy: {random_accuracy:.4f}, F1: {random_f1:.4f}, AUC: {random_auc:.4f if random_auc else 'N/A'}\")\n",
        "    \n",
        "    # Store best model\n",
        "    if grid_f1 > random_f1:\n",
        "        final_results[model_name] = {\n",
        "            'model': grid_model,\n",
        "            'method': 'GridSearchCV',\n",
        "            'accuracy': grid_accuracy,\n",
        "            'f1': grid_f1,\n",
        "            'auc': grid_auc,\n",
        "            'params': grid_results[model_name]['best_params']\n",
        "        }\n",
        "    else:\n",
        "        final_results[model_name] = {\n",
        "            'model': random_model,\n",
        "            'method': 'RandomizedSearchCV',\n",
        "            'accuracy': random_accuracy,\n",
        "            'f1': random_f1,\n",
        "            'auc': random_auc,\n",
        "            'params': random_results[model_name]['best_params']\n",
        "        }\n",
        "\n",
        "# Find the overall best model\n",
        "best_model_name = max(final_results.keys(), key=lambda x: final_results[x]['f1'])\n",
        "best_model = final_results[best_model_name]\n",
        "\n",
        "print(f\"\\nOverall Best Model: {best_model_name}\")\n",
        "print(f\"Method: {best_model['method']}\")\n",
        "print(f\"F1-Score: {best_model['f1']:.4f}\")\n",
        "print(f\"Accuracy: {best_model['accuracy']:.4f}\")\n",
        "print(f\"Best Parameters: {best_model['params']}\")\n",
        "\n",
        "# Save the best model\n",
        "import joblib\n",
        "joblib.dump(best_model['model'], 'models/best_model.pkl')\n",
        "print(f\"\\nBest model saved to 'models/best_model.pkl'\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
